{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import PIL\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='../Data'\n",
    "save_path = '../Preprocessed_Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    with open(os.path.join(save_path,file_name),'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# train_images = load_data('resized_train_images.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Data/train.jsonl\") as f:\n",
    "    train_lines = f.readlines()\n",
    "\n",
    "data_train = [eval(c) for c in train_lines]\n",
    "df_train = pd.DataFrame(data_train)\n",
    "\n",
    "with open(\"../Data/dev_seen.jsonl\") as f:\n",
    "    dev_seen_lines = f.readlines()\n",
    "\n",
    "data_dev_seen = [eval(c) for c in dev_seen_lines]\n",
    "df_dev_seen = pd.DataFrame(data_dev_seen)\n",
    "\n",
    "with open(\"../Data/test_seen.jsonl\") as f:\n",
    "    test_seen_lines = f.readlines()\n",
    "\n",
    "data_test_seen = [eval(c) for c in test_seen_lines]\n",
    "df_test_seen = pd.DataFrame(data_test_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, _ = train_test_split(df_train, test_size=0.7, random_state=42, stratify=df_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(df):\n",
    "    images = []\n",
    "    ls_exists = []\n",
    "    paths = df['img'].to_list()\n",
    "    for i in tqdm(range(len(paths))):\n",
    "        if not os.path.exists(path + \"/\" + paths[i]):\n",
    "            ls_exists.append(False)\n",
    "            continue\n",
    "        ls_exists.append(True)\n",
    "        img = np.array(PIL.Image.open(path + \"/\" + paths[i]))\n",
    "        images.append(img)\n",
    "    return images, ls_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e9278883ac4b9598da1be4cc32689f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_images, train_ls_exists = load_images(df_train)\n",
    "dev_seen_images, dev_seen_ls_exists = load_images(df_dev_seen)\n",
    "# test_seen_images, test_seen_ls_exists = load_images(df_test_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['img_exists'] = train_ls_exists\n",
    "df_dev_seen['img_exists'] = dev_seen_ls_exists\n",
    "# df_test_seen['img_exists'] = test_seen_ls_exists\n",
    "\n",
    "# df_train = df_train[df_train['img_exists'] == True]\n",
    "df_dev_seen = df_dev_seen[df_dev_seen['img_exists'] == True]\n",
    "# df_test_seen = df_test_seen[df_test_seen['img_exists'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>img_exists</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08291</td>\n",
       "      <td>img/08291.png</td>\n",
       "      <td>1</td>\n",
       "      <td>white people is this a shooting range</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80243</td>\n",
       "      <td>img/80243.png</td>\n",
       "      <td>1</td>\n",
       "      <td>mississippi wind chime</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>53046</td>\n",
       "      <td>img/53046.png</td>\n",
       "      <td>1</td>\n",
       "      <td>you've heard of elf on a shelf, now get ready for</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>80597</td>\n",
       "      <td>img/80597.png</td>\n",
       "      <td>1</td>\n",
       "      <td>look! it says it right here! we can fuck goats!</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>45368</td>\n",
       "      <td>img/45368.png</td>\n",
       "      <td>1</td>\n",
       "      <td>enough is enough children are more important t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>05439</td>\n",
       "      <td>img/05439.png</td>\n",
       "      <td>0</td>\n",
       "      <td>portable dishwasher slash sandwich maker</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>07528</td>\n",
       "      <td>img/07528.png</td>\n",
       "      <td>1</td>\n",
       "      <td>the latest and greatest. a truck that comes wi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>83675</td>\n",
       "      <td>img/83675.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm gonna be like phelps one day</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>37198</td>\n",
       "      <td>img/37198.png</td>\n",
       "      <td>0</td>\n",
       "      <td>when you're so relaxed you can feel yourself g...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>48670</td>\n",
       "      <td>img/48670.png</td>\n",
       "      <td>0</td>\n",
       "      <td>look at this sandwich maker club i found on wi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id            img  label  \\\n",
       "0    08291  img/08291.png      1   \n",
       "4    80243  img/80243.png      1   \n",
       "7    53046  img/53046.png      1   \n",
       "11   80597  img/80597.png      1   \n",
       "12   45368  img/45368.png      1   \n",
       "..     ...            ...    ...   \n",
       "488  05439  img/05439.png      0   \n",
       "493  07528  img/07528.png      1   \n",
       "495  83675  img/83675.png      0   \n",
       "496  37198  img/37198.png      0   \n",
       "497  48670  img/48670.png      0   \n",
       "\n",
       "                                                  text  img_exists  \n",
       "0                white people is this a shooting range        True  \n",
       "4                               mississippi wind chime        True  \n",
       "7    you've heard of elf on a shelf, now get ready for        True  \n",
       "11     look! it says it right here! we can fuck goats!        True  \n",
       "12   enough is enough children are more important t...        True  \n",
       "..                                                 ...         ...  \n",
       "488           portable dishwasher slash sandwich maker        True  \n",
       "493  the latest and greatest. a truck that comes wi...        True  \n",
       "495                   i'm gonna be like phelps one day        True  \n",
       "496  when you're so relaxed you can feel yourself g...        True  \n",
       "497  look at this sandwich maker club i found on wi...        True  \n",
       "\n",
       "[249 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 240 820 825\n"
     ]
    }
   ],
   "source": [
    "min_length = float('inf')\n",
    "min_breadth = float('inf')\n",
    "max_length = 0\n",
    "max_breadth = 0\n",
    "# for i in range(len(train_images)):\n",
    "#     if train_images[i].shape[0] < min_length:\n",
    "#         min_length = train_images[i].shape[0]\n",
    "#     if train_images[i].shape[1] < min_breadth:\n",
    "#         min_breadth = train_images[i].shape[1]\n",
    "#     if train_images[i].shape[0] > max_length:\n",
    "#         max_length = train_images[i].shape[0]\n",
    "#     if train_images[i].shape[1] > max_breadth:\n",
    "#         max_breadth = train_images[i].shape[1]\n",
    "for i in range(len(dev_seen_images)):\n",
    "    if dev_seen_images[i].shape[0] < min_length:\n",
    "        min_length = dev_seen_images[i].shape[0]\n",
    "    if dev_seen_images[i].shape[1] < min_breadth:\n",
    "        min_breadth = dev_seen_images[i].shape[1]\n",
    "    if dev_seen_images[i].shape[0] > max_length:\n",
    "        max_length = dev_seen_images[i].shape[0]\n",
    "    if dev_seen_images[i].shape[1] > max_breadth:\n",
    "        max_breadth = dev_seen_images[i].shape[1]\n",
    "# for i in range(len(test_seen_images)):\n",
    "#     if test_seen_images[i].shape[0] < min_length:\n",
    "#         min_length = test_seen_images[i].shape[0]\n",
    "#     if test_seen_images[i].shape[1] < min_breadth:\n",
    "#         min_breadth = test_seen_images[i].shape[1]\n",
    "#     if test_seen_images[i].shape[0] > max_length:\n",
    "#         max_length = test_seen_images[i].shape[0]\n",
    "#     if test_seen_images[i].shape[1] > max_breadth:\n",
    "#         max_breadth = test_seen_images[i].shape[1]\n",
    "print(min_length, min_breadth, max_length, max_breadth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_length = 224\n",
    "resnet_breadth = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images, length, breadth):\n",
    "    resized_images = []\n",
    "    for i in range(len(images)):\n",
    "        image_length = images[i].shape[0]\n",
    "        image_breadth = images[i].shape[1]\n",
    "        image_new = images[i]\n",
    "        if image_length > length:\n",
    "            image_new = image_new[:length, :, :]\n",
    "        elif image_length < length:\n",
    "            # repeat the image horizontally\n",
    "            extra = length - image_length\n",
    "            if extra < image_length:\n",
    "                chunk = image_new[image_length - extra:, :, :]\n",
    "            else:\n",
    "                number_of_full_chunks = int(extra / image_length)\n",
    "                remainder = extra % image_length\n",
    "                chunk = image_new\n",
    "                for j in range(number_of_full_chunks-1):\n",
    "                    chunk = np.concatenate((chunk, image_new), axis=0)\n",
    "                if remainder > 0:\n",
    "                    chunk = np.concatenate((chunk, image_new[:remainder, :, :]), axis=0)\n",
    "            image_new = np.concatenate((image_new, chunk), axis=0)\n",
    "        if image_breadth > breadth:\n",
    "            image_new = image_new[:, :breadth, :]\n",
    "        elif image_breadth < breadth:\n",
    "            # repeat the image vertically\n",
    "            extra = breadth - image_breadth\n",
    "            if extra < image_breadth:\n",
    "                chunk = image_new[:, image_breadth - extra:, :]\n",
    "            else:\n",
    "                number_of_full_chunks = int(extra / image_breadth)\n",
    "                remainder = extra % image_breadth\n",
    "                chunk = image_new\n",
    "                for j in range(number_of_full_chunks-1):\n",
    "                    chunk = np.concatenate((chunk, image_new), axis=1)\n",
    "                if remainder > 0:\n",
    "                    chunk = np.concatenate((chunk, image_new[:, :remainder, :]), axis=1)\n",
    "            image_new = np.concatenate((image_new, chunk), axis=1)\n",
    "        resized_images.append(image_new)\n",
    "    return resized_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resized_train_images = resize_images(train_images, resnet_length, resnet_breadth)\n",
    "resized_dev_seen_images = resize_images(dev_seen_images, resnet_length, resnet_breadth)\n",
    "# resized_test_seen_images = resize_images(test_seen_images, resnet_length, resnet_breadth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find image smaller than mean_length and mean_breadth\n",
    "# for i in range(len(train_images)):\n",
    "#     if train_images[i].shape[0] < resnet_length or train_images[i].shape[1] < resnet_breadth:\n",
    "#         print(train_images[i].shape)\n",
    "#         # plt.imshow(train_images[i])\n",
    "#         plt.imshow(resized_train_images[i])\n",
    "#         break\n",
    "\n",
    "for i in range(len(dev_seen_images)):\n",
    "    if dev_seen_images[i].shape[0] < resnet_length or dev_seen_images[i].shape[1] < resnet_breadth:\n",
    "        print(dev_seen_images[i].shape, resized_dev_seen_images[i].shape)\n",
    "        # plt.imshow(dev_seen_images[i])\n",
    "        plt.imshow(resized_dev_seen_images[i])\n",
    "        break\n",
    "\n",
    "# for i in range(len(test_seen_images)):\n",
    "#     if test_seen_images[i].shape[0] < resnet_length or test_seen_images[i].shape[1] < resnet_breadth:\n",
    "#         print(test_seen_images[i].shape, resized_test_seen_images[i].shape)\n",
    "#         # plt.imshow(test_seen_images[i])\n",
    "#         plt.imshow(resized_test_seen_images[i])\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "# Load the ResNet50 model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
    "# Set the model to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "# Define a function to get the ResNet embeddings for an image\n",
    "def get_resnet_embedding(image):\n",
    "    # Convert the image to a PIL Image object\n",
    "    image = Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Get the ResNet embeddings from penultimate layer\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet(image).squeeze()\n",
    "        \n",
    "\n",
    "    return embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize the images\n",
    "# def normalize_images(image):\n",
    "#     return image / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet_embedding_train_images = []\n",
    "# for i in range(len(resized_train_images)):\n",
    "#     resnet_embedding_train_images.append(get_resnet_embedding(resized_train_images[i]))\n",
    "\n",
    "resnet_embedding_dev_seen_images = []\n",
    "for i in range(len(resized_dev_seen_images)):\n",
    "    resnet_embedding_dev_seen_images.append(get_resnet_embedding(resized_dev_seen_images[i]))\n",
    "\n",
    "# resnet_embedding_test_seen_images = []\n",
    "# for i in range(len(resized_test_seen_images)):\n",
    "#     resnet_embedding_test_seen_images.append(get_resnet_embedding(resized_test_seen_images[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(resized_train_images)):\n",
    "#     resized_train_images[i] = normalize_images(resized_train_images[i])\n",
    "# for i in range(len(resized_dev_seen_images)):\n",
    "#     resized_dev_seen_images[i] = normalize_images(resized_dev_seen_images[i])\n",
    "# for i in range(len(resized_test_seen_images)):\n",
    "#     resized_test_seen_images[i] = normalize_images(resized_test_seen_images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_path + '/resnet_embedding_train_images.pkl', 'wb') as f:\n",
    "#     pickle.dump(resnet_embedding_train_images, f)\n",
    "\n",
    "with open(save_path + '/resnet_embedding_dev_seen_images.pkl', 'wb') as f:\n",
    "    pickle.dump(resnet_embedding_dev_seen_images, f)\n",
    "\n",
    "# with open(save_path + '/resnet_embedding_test_seen_images.pkl', 'wb') as f:\n",
    "#     pickle.dump(resnet_embedding_test_seen_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(save_path + \"/resized_train_images.pickle\", \"wb\") as f:\n",
    "#   pickle.dump(resized_train_images, f)\n",
    "\n",
    "with open(save_path + \"/resized_dev_seen_images.pickle\", \"wb\") as f:\n",
    "  pickle.dump(resized_dev_seen_images, f)\n",
    "\n",
    "# with open(save_path + \"/resized_test_seen_images.pickle\", \"wb\") as f:\n",
    "#   pickle.dump(resized_test_seen_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv(save_path + \"/df_train.csv\", index=False)\n",
    "# df_dev_seen.to_csv(save_path + \"/df_dev_seen.csv\", index=False)\n",
    "df_test_seen.to_csv(save_path + \"/df_test_seen.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
