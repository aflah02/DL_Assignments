{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Kyode\\clg\\DL_Assignments\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from modeling_lstm_seq2seq import LSTMSeq2Seq, Config\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiplicativeLR\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt16 (C:/Users/gener/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227)\n",
      "Found cached dataset wmt16 (C:/Users/gener/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227)\n",
      "Found cached dataset wmt16 (C:/Users/gener/.cache/huggingface/datasets/wmt16/de-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227)\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_dataset('wmt16', 'de-en', split='train')\n",
    "val_ds = load_dataset('wmt16', 'de-en', split='validation')\n",
    "test_ds = load_dataset('wmt16', 'de-en', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    input_size=32000,\n",
    "    embedding_size=1000,\n",
    "    hidden_size=1000,\n",
    "    num_layers=4,\n",
    "    vocab_size=50000,\n",
    "    dropout=0.2,\n",
    "    device=\"cuda\",\n",
    "    max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMSeq2Seq(config)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Kyode\\clg\\DL_Assignments\\venv\\lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(\n",
      "c:\\Kyode\\clg\\DL_Assignments\\venv\\lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    return [tok for tok in de_tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok for tok in en_tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4548885/4548885 [04:49<00:00, 15718.71it/s]\n",
      "100%|██████████| 4548885/4548885 [13:06<00:00, 5781.03it/s] \n"
     ]
    }
   ],
   "source": [
    "tokenized_train_eng = [tokenize_en(text['en']) for text in tqdm(train_ds['translation'])]\n",
    "tokenized_train_ger = [tokenize_de(text['de']) for text in tqdm(train_ds['translation'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2169/2169 [00:02<00:00, 904.36it/s] \n",
      "100%|██████████| 2169/2169 [00:00<00:00, 7856.64it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_val_eng = [tokenize_en(text['en']) for text in tqdm(val_ds['translation'])]\n",
    "tokenized_val_ger = [tokenize_de(text['de']) for text in tqdm(val_ds['translation'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4548885/4548885 [01:39<00:00, 45571.21it/s]\n",
      "100%|██████████| 4548885/4548885 [01:30<00:00, 50240.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# eng_vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_train_eng, max_tokens = 50000)\n",
    "# ger_vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_train_ger, max_tokens = 50000)\n",
    "\n",
    "# create a vocabulary of the training samples for only the top 50000 most common words without torchtext\n",
    "\n",
    "eng_vocab = {}\n",
    "ger_vocab = {}\n",
    "\n",
    "for text in tqdm(tokenized_train_eng):\n",
    "    for word in text:\n",
    "        if word in eng_vocab:\n",
    "            eng_vocab[word] += 1\n",
    "        else:\n",
    "            eng_vocab[word] = 1\n",
    "\n",
    "for text in tqdm(tokenized_train_ger):\n",
    "    for word in text:\n",
    "        if word in ger_vocab:\n",
    "            ger_vocab[word] += 1\n",
    "        else:\n",
    "            ger_vocab[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab = {k: v for k, v in sorted(eng_vocab.items(), key=lambda item: item[1], reverse=True)}\n",
    "ger_vocab = {k: v for k, v in sorted(ger_vocab.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab = dict(list(eng_vocab.items())[:50000])\n",
    "ger_vocab = dict(list(ger_vocab.items())[:50000])\n",
    "\n",
    "eng_vocab = {k: i+2 for i, k in enumerate(eng_vocab.keys())}\n",
    "ger_vocab = {k: i+2 for i, k in enumerate(ger_vocab.keys())}\n",
    "\n",
    "eng_vocab['<unk>'] = 0\n",
    "ger_vocab['<unk>'] = 0\n",
    "\n",
    "eng_vocab['<eos>'] = 1\n",
    "ger_vocab['<eos>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_eng(text):\n",
    "    encoded = []\n",
    "    for token in text:\n",
    "        try:\n",
    "            encoded.append(eng_vocab[token])\n",
    "        except:\n",
    "            encoded.append(eng_vocab['<unk>'])\n",
    "    encoded.append(eng_vocab['<eos>'])\n",
    "    return encoded\n",
    "\n",
    "def encode_ger(text):\n",
    "    encoded = []\n",
    "    for token in text:\n",
    "        try:\n",
    "            encoded.append(ger_vocab[token])\n",
    "        except:\n",
    "            encoded.append(ger_vocab['<unk>'])\n",
    "    encoded.append(ger_vocab['<eos>'])\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4548885/4548885 [13:33<00:00, 5589.26it/s]  \n",
      "100%|██████████| 4548885/4548885 [07:08<00:00, 10626.79it/s] \n"
     ]
    }
   ],
   "source": [
    "encoded_train_eng = [encode_eng(text) for text in tqdm(tokenized_train_eng)]\n",
    "encoded_train_ger = [encode_ger(text) for text in tqdm(tokenized_train_ger)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2169/2169 [00:00<00:00, 5389.40it/s]\n",
      "100%|██████████| 2169/2169 [00:00<00:00, 490560.55it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_val_eng = [encode_eng(text) for text in tqdm(tokenized_val_eng)]\n",
    "encoded_val_ger = [encode_ger(text) for text in tqdm(tokenized_val_ger)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataloader = DataLoader(list(zip(encoded_train_ger, encoded_train_eng)), batch_size=BATCH_SIZE, shuffle=True)\n",
    "tokenized_val_dataloader = DataLoader(list(zip(encoded_val_ger, encoded_val_eng)), batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        src = batch[0].to(device)\n",
    "        trg = batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output = output[1:].view(-1, output.shape[2])\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            src = batch[0].to(device)\n",
    "            trg = batch[1].to(device)\n",
    "            output = model(src, trg, 0)\n",
    "            output = output[1:].view(-1, output.shape[2])\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train(model, tokenized_train_dataloader, optimizer, criterion, config.device)\n",
    "    val_loss = evaluate(model, tokenized_val_dataloader, criterion, config.device)\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
